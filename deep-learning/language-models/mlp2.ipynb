{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "'''\n",
    "The MLP implemented in this notebook is based on the design from\n",
    "the paper 'A Neural Probabilistic Language Model' (Bengio et al. 2003)\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Kaiming initialisation is implemented based on 'Delving Deep into Rectifiers'\n",
    "(Kaiming et al. 2015) https://arxiv.org/pdf/1502.01852.pdf\n",
    "\n",
    "Batch normalisation is implemented is based on work from the paper 'Batch\n",
    "Normalization:  Accelerating Deep Network Training by Reducing Internal \n",
    "Covariate Shift' (Ioffe et al. 2015) https://arxiv.org/pdf/1502.03167.pdf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for i, s in enumerate(chars)}\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, nin: int, nout: int, bias: bool=True):\n",
    "        kaiming_init = 1 / nin**0.5\n",
    "        self.weights = torch.randn((nin, nout)) * kaiming_init\n",
    "        self.bias = torch.zeros(nout) if bias else None\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1D:\n",
    "\n",
    "    def __init__(self, dims: int, epsilon: float=1e-5, momentum: float=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Parameters.\n",
    "        self.gamma = torch.ones(dims)\n",
    "        self.beta = torch.zeros(dims)\n",
    "        self.mean_live = torch.zeros(dims)\n",
    "        self.varience_live = torch.ones(dims)\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        # Forward pass.\n",
    "        if self.training:\n",
    "            x_mean = x.mean(0, keepdim=True) # Batch mean.\n",
    "            x_varience = x.var(0, keepdim=True) # Batch varience.\n",
    "        else:\n",
    "            x_mean = self.mean_live\n",
    "            x_varience = self.varience_live\n",
    "        x_norm = (x - x_mean) / torch.sqrt(x_varience + self.epsilon) # Normalise to unit varience.\n",
    "        self.out = self.gamma * x_norm + self.beta\n",
    "        # Update parameters.\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.mean_live = (1 - self.momentum) * self.mean_live + self.momentum * x_mean\n",
    "                self.varience_live = (1 - self.momentum) * self.varience_live + self.momentum * x_varience\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x: list[float]):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "block_size = 3 # Context length (characters).\n",
    "embedding_dims = 10 # Number of dimensions for the embedding space.\n",
    "batch_size = 32 # Number of examples to process at a time in training.\n",
    "hidden_layer_size = 100 # Number of neurons in the hidden layer.\n",
    "init_lr = 0.1 # Initial learning rate.\n",
    "final_lr = 0.01 # Final learning rate.\n",
    "max_steps = 200000\n",
    "\n",
    "def build_dataset(words: list[str]) -> (torch.Tensor, torch.Tensor):\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        context = [0] * block_size # Padding the context with initial '.' characters.\n",
    "        for char in word + '.':\n",
    "            idx = stoi[char]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            context = context[1:] + [idx] # Update context.\n",
    "\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "x_train, y_train = build_dataset(words[:n1])\n",
    "x_val, y_val = build_dataset(words[n1:n2])\n",
    "x_test, y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding matrix.\n",
    "C = torch.randn((vocab_size, embedding_dims))\n",
    "\n",
    "# MLP.\n",
    "layers = [\n",
    "    Linear(embedding_dims * block_size, hidden_layer_size), BatchNorm1D(hidden_layer_size), Tanh(),\n",
    "    Linear(hidden_layer_size, hidden_layer_size), BatchNorm1D(hidden_layer_size), Tanh(),\n",
    "    Linear(hidden_layer_size, hidden_layer_size), BatchNorm1D(hidden_layer_size), Tanh(),\n",
    "    Linear(hidden_layer_size, hidden_layer_size), BatchNorm1D(hidden_layer_size), Tanh(),\n",
    "    Linear(hidden_layer_size, hidden_layer_size), BatchNorm1D(hidden_layer_size), Tanh(),\n",
    "    Linear(hidden_layer_size, vocab_size), BatchNorm1D(vocab_size)\n",
    "]\n",
    "\n",
    "# Initialisations.\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1 # Make last layer less confident.\n",
    "    for layer in layers[:-1]: # For all other layers, apply gain.\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weights *= 5/3\n",
    "\n",
    "params = [C] + [param for layer in layers for param in layer.parameters()]\n",
    "for param in params:\n",
    "    param.requires_grad = True\n",
    "    \n",
    "print(f'Number of parameters: {sum(param.nelement() for param in params)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent.\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # Constructing batches.\n",
    "    idx = torch.randint(0, x_train.shape[0], (batch_size, ))\n",
    "\n",
    "    # Forward pass.\n",
    "    embedding = C[x_train[idx]] # Embed characters into vectors.\n",
    "    x = embedding.view(embedding.size(0), -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = torch.nn.functional.cross_entropy(x, y_train[idx])\n",
    "\n",
    "    # Backward pass.\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Stochastic gradient descent.\n",
    "    lr = init_lr if i < (max_steps / 2) else final_lr\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_loss(split: str) -> None:\n",
    "    ''' Evaluates the model on the chosen split. '''\n",
    "\n",
    "    x, y = {\n",
    "        'train': (x_train, y_train),\n",
    "        'val': (x_val, y_val),\n",
    "        'test': (x_test, y_test)\n",
    "    }[split]\n",
    "    \n",
    "    # Forward pass.\n",
    "    embedding = C[x]\n",
    "    x = embedding.view(embedding.size(0), -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = torch.nn.functional.cross_entropy(x, y)\n",
    "    print(f'{split.capitalize()} Loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layers to evaluation mode.\n",
    "for layer in layers:\n",
    "  layer.training = False\n",
    "\n",
    "split_loss('test')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model.\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size # Initialise context to '...'\n",
    "\n",
    "    while True:\n",
    "        # Forward pass.\n",
    "        embedding = C[torch.tensor([context])]\n",
    "        x = embedding.view(embedding.size(0), -1)\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        logits = x\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample from the distribution.\n",
    "        idx = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        context = context[1:] + [idx] # Shift the context window.\n",
    "        out.append(itos[idx])\n",
    "        if idx == 0: # If we sample '.', stop.\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
